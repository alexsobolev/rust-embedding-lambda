{"text": "Rust on AWS Lambda provides excellent performance for machine learning workloads. The combination of Rust's zero-cost abstractions and AWS Lambda's serverless architecture creates a powerful platform for deploying ML models. ONNX Runtime enables efficient inference with optimized operators for ARM64 Graviton processors. Matryoshka representation learning allows flexible embedding dimensions, supporting 128, 256, 512, and 768-dimensional vectors from a single model. The quantized model reduces memory footprint while maintaining high accuracy. Mean pooling over token embeddings creates document-level representations. L2 normalization enables efficient cosine similarity computation through dot products. This longer text tests the system's ability to handle more complex tokenization and inference scenarios with hundreds of tokens. Additional context helps evaluate how the model performs with realistic document lengths commonly seen in production embedding workloads.", "size": 768}
